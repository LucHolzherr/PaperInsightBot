[
    {
        "name": "V. Koltun",
        "affiliations": [],
        "citation_count": 71643,
        "h_index": 109,
        "papers": [
            {
                "title": "Multi-Scale Context Aggregation by Dilated Convolutions",
                "year": 2015,
                "citations": 8429,
                "abstract": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy.",
                "abstract_summary": "This research focuses on improving semantic segmentation models by introducing a new convolutional network module specifically designed for dense prediction. The module uses dilated convolutions to aggregate multi-scale contextual information without losing resolution. The study also explores the adaptation of image classification networks to dense prediction, suggesting that simplifying the adapted network can enhance accuracy."
            },
            {
                "title": "CARLA: An Open Urban Driving Simulator",
                "year": 2017,
                "citations": 5127,
                "abstract": "We introduce CARLA, an open-source simulator for autonomous driving research. CARLA has been developed from the ground up to support development, training, and validation of autonomous urban driving systems. In addition to open-source code and protocols, CARLA provides open digital assets (urban layouts, buildings, vehicles) that were created for this purpose and can be used freely. The simulation platform supports flexible specification of sensor suites and environmental conditions. We use CARLA to study the performance of three approaches to autonomous driving: a classic modular pipeline, an end-to-end model trained via imitation learning, and an end-to-end model trained via reinforcement learning. The approaches are evaluated in controlled scenarios of increasing difficulty, and their performance is examined via metrics provided by CARLA, illustrating the platform's utility for autonomous driving research. The supplementary video can be viewed at this https URL",
                "abstract_summary": "The research introduces CARLA, an open-source simulator developed for autonomous driving research. The simulator supports the development, training, and validation of autonomous urban driving systems, providing open digital assets and flexible sensor suites. The study uses CARLA to evaluate three autonomous driving approaches, demonstrating the platform's utility for such research."
            },
            {
                "title": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling",
                "year": 2018,
                "citations": 4776,
                "abstract": "For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at this http URL .",
                "abstract_summary": "The research systematically evaluates convolutional and recurrent architectures for sequence modeling across various tasks and datasets. The results show that a simple convolutional architecture surpasses traditional recurrent networks like LSTMs in performance and memory efficiency. The study suggests reconsidering the common association of sequence modeling with recurrent networks and proposes convolutional networks as a better starting point for such tasks."
            },
            {
                "title": "Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials",
                "year": 2011,
                "citations": 3451,
                "abstract": "Most state-of-the-art techniques for multi-class image segmentation and labeling use conditional random fields defined over pixels or image regions. While region-level models often feature dense pairwise connectivity, pixel-level models are considerably larger and have only permitted sparse graph structures. In this paper, we consider fully connected CRF models defined on the complete set of pixels in an image. The resulting graphs have billions of edges, making traditional inference algorithms impractical. Our main contribution is a highly efficient approximate inference algorithm for fully connected CRF models in which the pairwise edge potentials are defined by a linear combination of Gaussian kernels. Our experiments demonstrate that dense connectivity at the pixel level substantially improves segmentation and labeling accuracy.",
                "abstract_summary": "This research focuses on multi-class image segmentation and labeling using fully connected Conditional Random Fields (CRF) models defined on all pixels in an image. The main contribution is a highly efficient approximate inference algorithm for these models, where pairwise edge potentials are defined by a linear combination of Gaussian kernels. The study found that dense connectivity at the pixel level significantly improves segmentation and labeling accuracy."
            },
            {
                "title": "Direct Sparse Odometry",
                "year": 2016,
                "citations": 2515,
                "abstract": "Direct Sparse Odometry (DSO) is a visual odometry method based on a novel, highly accurate sparse and direct structure and motion formulation. It combines a fully direct probabilistic model (minimizing a photometric error) with consistent, joint optimization of all model parameters, including geometry-represented as inverse depth in a reference frame-and camera motion. This is achieved in real time by omitting the smoothness prior used in other direct methods and instead sampling pixels evenly throughout the images. Since our method does not depend on keypoint detectors or descriptors, it can naturally sample pixels from across all image regions that have intensity gradient, including edges or smooth intensity variations on essentially featureless walls. The proposed model integrates a full photometric calibration, accounting for exposure time, lens vignetting, and non-linear response functions. We thoroughly evaluate our method on three different datasets comprising several hours of video. The experiments show that the presented approach significantly outperforms state-of-the-art direct and indirect methods in a variety of real-world settings, both in terms of tracking accuracy and robustness.",
                "abstract_summary": "The research focuses on Direct Sparse Odometry (DSO), a visual odometry method that utilizes a fully direct probabilistic model and optimizes all model parameters for real-time operation. The method, which doesn't rely on keypoint detectors or descriptors, samples pixels from all image regions with an intensity gradient, integrating full photometric calibration. The study's experiments on three different datasets demonstrate that DSO significantly surpasses other direct and indirect methods in tracking accuracy and robustness."
            },
            {
                "title": "Playing for Data: Ground Truth from Computer Games",
                "year": 2016,
                "citations": 2002,
                "abstract": "Recent progress in computer vision has been driven by high-capacity models trained on large datasets. Unfortunately, creating large datasets with pixel-level labels has been extremely costly due to the amount of human effort required. In this paper, we present an approach to rapidly creating pixel-accurate semantic label maps for images extracted from modern computer games. Although the source code and the internal operation of commercial games are inaccessible, we show that associations between image patches can be reconstructed from the communication between the game and the graphics hardware. This enables rapid propagation of semantic labels within and across images synthesized by the game, with no access to the source code or the content. We validate the presented approach by producing dense pixel-level semantic annotations for 25 thousand images synthesized by a photorealistic open-world computer game. Experiments on semantic segmentation datasets show that using the acquired data to supplement real-world images significantly increases accuracy and that the acquired data enables reducing the amount of hand-labeled real-world data: models trained with game data and just \\(\\tfrac{1}{3}\\) of the CamVid training set outperform models trained on the complete CamVid training set.",
                "abstract_summary": "The study presents a method for creating pixel-accurate semantic label maps for images from computer games, bypassing the need for source code access. The technique involves reconstructing associations between image patches from game-graphics hardware communication, allowing rapid label propagation. The approach, validated on 25,000 game-synthesized images, improves accuracy in semantic segmentation datasets and reduces the need for hand-labeled real-world data."
            },
            {
                "title": "Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-Shot Cross-Dataset Transfer",
                "year": 2019,
                "citations": 1774,
                "abstract": "The success of monocular depth estimation relies on large and diverse training sets. Due to the challenges associated with acquiring dense ground-truth depth across different environments at scale, a number of datasets with distinct characteristics and biases have emerged. We develop tools that enable mixing multiple datasets during training, even if their annotations are incompatible. In particular, we propose a robust training objective that is invariant to changes in depth range and scale, advocate the use of principled multi-objective learning to combine data from different sources, and highlight the importance of pretraining encoders on auxiliary tasks. Armed with these tools, we experiment with five diverse training datasets, including a new, massive data source: 3D films. To demonstrate the generalization power of our approach we use zero-shot cross-dataset transfer, i.e. we evaluate on datasets that were not seen during training. The experiments confirm that mixing data from complementary sources greatly improves monocular depth estimation. Our approach clearly outperforms competing methods across diverse datasets, setting a new state of the art for monocular depth estimation.",
                "abstract_summary": "The research focuses on improving monocular depth estimation by using multiple diverse training datasets, including 3D films, even if their annotations are incompatible. The researchers developed a robust training objective that is invariant to changes in depth range and scale, and highlighted the importance of pretraining encoders on auxiliary tasks. The study found that mixing data from various sources significantly enhances monocular depth estimation, outperforming other methods and setting a new standard in the field."
            },
            {
                "title": "Vision Transformers for Dense Prediction",
                "year": 2021,
                "citations": 1710,
                "abstract": "We introduce dense prediction transformers, an architecture that leverages vision transformers in place of convolutional networks as a backbone for dense prediction tasks. We assemble tokens from various stages of the vision transformer into image-like representations at various resolutions and progressively combine them into full-resolution predictions using a convolutional decoder. The transformer backbone processes representations at a constant and relatively high resolution and has a global receptive field at every stage. These properties allow the dense prediction transformer to provide finer-grained and more globally coherent predictions when compared to fully-convolutional networks. Our experiments show that this architecture yields substantial improvements on dense prediction tasks, especially when a large amount of training data is available. For monocular depth estimation, we observe an improvement of up to 28% in relative performance when compared to a state-of-the-art fully-convolutional network. When applied to semantic segmentation, dense prediction transformers set a new state of the art on ADE20K with 49.02% mIoU. We further show that the architecture can be fine-tuned on smaller datasets such as NYUv2, KITTI, and Pascal Context where it also sets the new state of the art. Our models are available at https://github.com/intel-isl/DPT.",
                "abstract_summary": "The study introduces dense prediction transformers, an architecture that uses vision transformers instead of convolutional networks for dense prediction tasks. The architecture provides finer-grained and globally coherent predictions, showing significant improvements in tasks like monocular depth estimation and semantic segmentation, especially with large training data. The architecture can also be fine-tuned on smaller datasets, setting new state-of-the-art results."
            },
            {
                "title": "Dilated Residual Networks",
                "year": 2017,
                "citations": 1617,
                "abstract": "Convolutional networks for image classification progressively reduce resolution until the image is represented by tiny feature maps in which the spatial structure of the scene is no longer discernible. Such loss of spatial acuity can limit image classification accuracy and complicate the transfer of the model to downstream applications that require detailed scene understanding. These problems can be alleviated by dilation, which increases the resolution of output feature maps without reducing the receptive field of individual neurons. We show that dilated residual networks (DRNs) outperform their non-dilated counterparts in image classification without increasing the models depth or complexity. We then study gridding artifacts introduced by dilation, develop an approach to removing these artifacts (degridding), and show that this further increases the performance of DRNs. In addition, we show that the accuracy advantage of DRNs is further magnified in downstream applications such as object localization and semantic segmentation.",
                "abstract_summary": "The research explores the use of dilated residual networks (DRNs) in image classification, which enhances the resolution of output feature maps without reducing the receptive field of neurons. The study also addresses gridding artifacts caused by dilation and presents a degridding method to improve DRN performance. The research concludes that DRNs not only improve image classification but also enhance object localization and semantic segmentation."
            },
            {
                "title": "Open3D: A Modern Library for 3D Data Processing",
                "year": 2018,
                "citations": 1603,
                "abstract": "Open3D is an open-source library that supports rapid development of software that deals with 3D data. The Open3D frontend exposes a set of carefully selected data structures and algorithms in both C++ and Python. The backend is highly optimized and is set up for parallelization. Open3D was developed from a clean slate with a small and carefully considered set of dependencies. It can be set up on different platforms and compiled from source with minimal effort. The code is clean, consistently styled, and maintained via a clear code review mechanism. Open3D has been used in a number of published research projects and is actively deployed in the cloud. We welcome contributions from the open-source community.",
                "abstract_summary": "The research focuses on Open3D, an open-source library designed for rapid development of 3D data software. The library, optimized for parallelization, offers selected data structures and algorithms in C++ and Python. Open3D, used in various research projects and cloud deployments, encourages contributions from the open-source community."
            }
        ]
    },
    {
        "name": "Alexey Bochkovskiy",
        "affiliations": [
            "Apple"
        ],
        "citation_count": 20153,
        "h_index": 6,
        "papers": [
            {
                "title": "YOLOv4: Optimal Speed and Accuracy of Object Detection",
                "year": 2020,
                "citations": 12198,
                "abstract": "There are a huge number of features which are said to improve Convolutional Neural Network (CNN) accuracy. Practical testing of combinations of such features on large datasets, and theoretical justification of the result, is required. Some features operate on certain models exclusively and for certain problems exclusively, or only for small-scale datasets; while some features, such as batch-normalization and residual-connections, are applicable to the majority of models, tasks, and datasets. We assume that such universal features include Weighted-Residual-Connections (WRC), Cross-Stage-Partial-connections (CSP), Cross mini-Batch Normalization (CmBN), Self-adversarial-training (SAT) and Mish-activation. We use new features: WRC, CSP, CmBN, SAT, Mish activation, Mosaic data augmentation, CmBN, DropBlock regularization, and CIoU loss, and combine some of them to achieve state-of-the-art results: 43.5% AP (65.7% AP50) for the MS COCO dataset at a realtime speed of ~65 FPS on Tesla V100. Source code is at this https URL",
                "abstract_summary": "The research explores various features said to enhance Convolutional Neural Network (CNN) accuracy, focusing on universal features like Weighted-Residual-Connections (WRC), Cross-Stage-Partial-connections (CSP), Cross mini-Batch Normalization (CmBN), Self-adversarial-training (SAT) and Mish-activation. New features including WRC, CSP, CmBN, SAT, Mish activation, Mosaic data augmentation, CmBN, DropBlock regularization, and CIoU loss are utilized and combined. The application of these features achieved state-of-the-art results on the MS COCO dataset with a real-time speed of ~65 FPS on Tesla V100."
            },
            {
                "title": "YOLOv7: Trainable Bag-of-Freebies Sets New State-of-the-Art for Real-Time Object Detectors",
                "year": 2022,
                "citations": 6440,
                "abstract": "Real-time object detection is one of the most important research topics in computer vision. As new approaches regarding architecture optimization and training optimization are continually being developed, we have found two research topics that have spawned when dealing with these latest state-of-the-art methods. To address the topics, we propose a trainable bag-of-freebies oriented solution. We combine the flexible and efficient training tools with the proposed architecture and the compound scaling method. YOLOv7 surpasses all known object detectors in both speed and accuracy in the range from 5 FPS to 120 FPS and has the highest accuracy 56.8% AP among all known real-time object detectors with 30 FPS or higher on GPU V100. Source code is released in https://github.com/WongKinYiu/yolov7.",
                "abstract_summary": "The research focuses on improving real-time object detection in computer vision using a trainable bag-of-freebies oriented solution. The proposed solution, YOLOv7, combines efficient training tools, a new architecture, and a compound scaling method. The study claims YOLOv7 outperforms all known object detectors in speed and accuracy, achieving the highest accuracy of 56.8% AP among detectors with 30 FPS or higher on GPU V100."
            },
            {
                "title": "Vision Transformers for Dense Prediction",
                "year": 2021,
                "citations": 1710,
                "abstract": "We introduce dense prediction transformers, an architecture that leverages vision transformers in place of convolutional networks as a backbone for dense prediction tasks. We assemble tokens from various stages of the vision transformer into image-like representations at various resolutions and progressively combine them into full-resolution predictions using a convolutional decoder. The transformer backbone processes representations at a constant and relatively high resolution and has a global receptive field at every stage. These properties allow the dense prediction transformer to provide finer-grained and more globally coherent predictions when compared to fully-convolutional networks. Our experiments show that this architecture yields substantial improvements on dense prediction tasks, especially when a large amount of training data is available. For monocular depth estimation, we observe an improvement of up to 28% in relative performance when compared to a state-of-the-art fully-convolutional network. When applied to semantic segmentation, dense prediction transformers set a new state of the art on ADE20K with 49.02% mIoU. We further show that the architecture can be fine-tuned on smaller datasets such as NYUv2, KITTI, and Pascal Context where it also sets the new state of the art. Our models are available at https://github.com/intel-isl/DPT.",
                "abstract_summary": "The study introduces Dense Prediction Transformers, a new architecture that uses vision transformers instead of convolutional networks for dense prediction tasks. The architecture provides finer, globally coherent predictions and shows significant improvements in tasks like monocular depth estimation and semantic segmentation, setting new state-of-the-art results on several datasets. The models are available for use and further research on GitHub."
            },
            {
                "title": "Scaled-YOLOv4: Scaling Cross Stage Partial Network",
                "year": 2020,
                "citations": 1143,
                "abstract": "We show that the YOLOv4 object detection neural network based on the CSP approach, scales both up and down and is applicable to small and large networks while maintaining optimal speed and accuracy. We propose a network scaling approach that modifies not only the depth, width, resolution, but also structure of the network. YOLOv4-large model achieves state-of-the-art results: 55.5% AP (73.4% AP50) for the MS COCO dataset at a speed of ~ 16 FPS on Tesla V100, while with the test time augmentation, YOLOv4-large achieves 56.0% AP (73.3 AP50). To the best of our knowledge, this is currently the highest accuracy on the COCO dataset among any published work. The YOLOv4-tiny model achieves 22.0% AP (42.0% AP50) at a speed of ~443 FPS on RTX 2080Ti, while by using TensorRT, batch size = 4 and FP16-precision the YOLOv4-tiny achieves 1774 FPS.",
                "abstract_summary": "The study demonstrates the scalability of the YOLOv4 object detection neural network based on the CSP approach, which can be applied to both small and large networks while maintaining optimal speed and accuracy. A network scaling approach is proposed that modifies the depth, width, resolution, and structure of the network. The YOLOv4-large model achieves state-of-the-art results on the MS COCO dataset, while the YOLOv4-tiny model achieves high speed."
            },
            {
                "title": "Depth Pro: Sharp Monocular Metric Depth in Less Than a Second",
                "year": 2024,
                "citations": 106,
                "abstract": "We present a foundation model for zero-shot metric monocular depth estimation. Our model, Depth Pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. The predictions are metric, with absolute scale, without relying on the availability of metadata such as camera intrinsics. And the model is fast, producing a 2.25-megapixel depth map in 0.3 seconds on a standard GPU. These characteristics are enabled by a number of technical contributions, including an efficient multi-scale vision transformer for dense prediction, a training protocol that combines real and synthetic datasets to achieve high metric accuracy alongside fine boundary tracing, dedicated evaluation metrics for boundary accuracy in estimated depth maps, and state-of-the-art focal length estimation from a single image. Extensive experiments analyze specific design choices and demonstrate that Depth Pro outperforms prior work along multiple dimensions. We release code and weights at https://github.com/apple/ml-depth-pro",
                "abstract_summary": "The study introduces a model, Depth Pro, for zero-shot metric monocular depth estimation, capable of synthesizing high-resolution, metric depth maps quickly without requiring metadata. The model's efficiency is attributed to several technical contributions such as a multi-scale vision transformer, a training protocol combining real and synthetic datasets, and dedicated evaluation metrics. The research claims Depth Pro outperforms previous models, with extensive experiments provided to support this claim."
            },
            {
                "title": "Non-deep Networks",
                "year": 2021,
                "citations": 71,
                "abstract": "Depth is the hallmark of deep neural networks. But more depth means more sequential computation and higher latency. This begs the question -- is it possible to build high-performing\"non-deep\"neural networks? We show that it is. To do so, we use parallel subnetworks instead of stacking one layer after another. This helps effectively reduce depth while maintaining high performance. By utilizing parallel substructures, we show, for the first time, that a network with a depth of just 12 can achieve top-1 accuracy over 80% on ImageNet, 96% on CIFAR10, and 81% on CIFAR100. We also show that a network with a low-depth (12) backbone can achieve an AP of 48% on MS-COCO. We analyze the scaling rules for our design and show how to increase performance without changing the network's depth. Finally, we provide a proof of concept for how non-deep networks could be used to build low-latency recognition systems. Code is available at https://github.com/imankgoyal/NonDeepNetworks.",
                "abstract_summary": "The research explores the construction of high-performing \"non-deep\" neural networks using parallel subnetworks, effectively reducing depth while maintaining performance. The study demonstrates that a network with a depth of 12 can achieve high accuracy on ImageNet, CIFAR10, and CIFAR100, and a good AP on MS-COCO. The paper also provides a proof of concept for using non-deep networks to build low-latency recognition systems."
            }
        ]
    },
    {
        "name": "Stephan R. Richter",
        "affiliations": [
            "Intel Labs"
        ],
        "citation_count": 3496,
        "h_index": 11,
        "papers": [
            {
                "title": "Playing for Data: Ground Truth from Computer Games",
                "year": 2016,
                "citations": 2002,
                "abstract": "Recent progress in computer vision has been driven by high-capacity models trained on large datasets. Unfortunately, creating large datasets with pixel-level labels has been extremely costly due to the amount of human effort required. In this paper, we present an approach to rapidly creating pixel-accurate semantic label maps for images extracted from modern computer games. Although the source code and the internal operation of commercial games are inaccessible, we show that associations between image patches can be reconstructed from the communication between the game and the graphics hardware. This enables rapid propagation of semantic labels within and across images synthesized by the game, with no access to the source code or the content. We validate the presented approach by producing dense pixel-level semantic annotations for 25 thousand images synthesized by a photorealistic open-world computer game. Experiments on semantic segmentation datasets show that using the acquired data to supplement real-world images significantly increases accuracy and that the acquired data enables reducing the amount of hand-labeled real-world data: models trained with game data and just \\(\\tfrac{1}{3}\\) of the CamVid training set outperform models trained on the complete CamVid training set.",
                "abstract_summary": "The study presents a method for quickly creating pixel-accurate semantic label maps from images extracted from computer games, bypassing the need for source code access. The approach allows for rapid propagation of semantic labels within and across game-synthesized images. The research validates this approach by enhancing accuracy in semantic segmentation datasets and reducing the need for hand-labeled real-world data."
            },
            {
                "title": "Playing for Benchmarks",
                "year": 2017,
                "citations": 467,
                "abstract": "We present a benchmark suite for visual perception. The benchmark is based on more than 250K high-resolution video frames, all annotated with ground-truth data for both low-level and high-level vision tasks, including optical flow, semantic instance segmentation, object detection and tracking, object-level 3D scene layout, and visual odometry. Ground-truth data for all tasks is available for every frame. The data was collected while driving, riding, and walking a total of 184 kilometers in diverse ambient conditions in a realistic virtual world. To create the benchmark, we have developed a new approach to collecting ground-truth data from simulated worlds without access to their source code or content. We conduct statistical analyses that show that the composition of the scenes in the benchmark closely matches the composition of corresponding physical environments. The realism of the collected data is further validated via perceptual experiments. We analyze the performance of state-of-the-art methods for multiple tasks, providing reference baselines and highlighting challenges for future research.",
                "abstract_summary": "The research presents a benchmark suite for visual perception, comprising over 250K high-resolution video frames annotated with ground-truth data for various vision tasks, collected from a realistic virtual world. A novel method was developed to collect this data from simulated worlds without needing source code access. The study validates the realism of the data through perceptual experiments and statistical analyses, and evaluates the performance of current methods, providing a reference for future research."
            },
            {
                "title": "What Do Single-View 3D Reconstruction Networks Learn?",
                "year": 2019,
                "citations": 427,
                "abstract": "Convolutional networks for single-view object reconstruction have shown impressive performance and have become a popular subject of research. All existing techniques are united by the idea of having an encoder-decoder network that performs non-trivial reasoning about the 3D structure of the output space. In this work, we set up two alternative approaches that perform image classification and retrieval respectively. These simple baselines yield better results than state-of-the-art methods, both qualitatively and quantitatively. We show that encoder-decoder methods are statistically indistinguishable from these baselines, thus indicating that the current state of the art in single-view object reconstruction does not actually perform reconstruction but image classification. We identify aspects of popular experimental procedures that elicit this behavior and discuss ways to improve the current state of research.",
                "abstract_summary": "The study critiques current techniques in single-view object reconstruction, arguing that they perform image classification rather than actual reconstruction. The researchers propose two alternative approaches for image classification and retrieval, which outperform existing methods. They also suggest improvements for experimental procedures to enhance the quality of research in this field."
            },
            {
                "title": "MeshSDF: Differentiable Iso-Surface Extraction",
                "year": 2020,
                "citations": 148,
                "abstract": "Geometric Deep Learning has recently made striking progress with the advent of continuous Deep Implicit Fields. They allow for detailed modeling of watertight surfaces of arbitrary topology while not relying on a 3D Euclidean grid, resulting in a learnable parameterization that is not limited in resolution. \nUnfortunately, these methods are often not suitable for applications that require an explicit mesh-based surface representation because converting an implicit field to such a representation relies on the Marching Cubes algorithm, which cannot be differentiated with respect to the underlying implicit field. \nIn this work, we remove this limitation and introduce a differentiable way to produce explicit surface mesh representations from Deep Signed Distance Functions. Our key insight is that by reasoning on how implicit field perturbations impact local surface geometry, one can ultimately differentiate the 3D location of surface samples with respect to the underlying deep implicit field. We exploit this to define MeshSDF, an end-to-end differentiable mesh representation which can vary its topology. \nWe use two different applications to validate our theoretical insight: Single-View Reconstruction via Differentiable Rendering and Physically-Driven Shape Optimization. In both cases our differentiable parameterization gives us an edge over state-of-the-art algorithms.",
                "abstract_summary": "The research introduces a differentiable method, MeshSDF, for creating explicit surface mesh representations from Deep Signed Distance Functions, overcoming limitations of current geometric deep learning techniques. The method allows for differentiation of the 3D location of surface samples with respect to the underlying deep implicit field. The effectiveness of this approach is validated through Single-View Reconstruction via Differentiable Rendering and Physically-Driven Shape Optimization applications, where it outperforms existing algorithms."
            },
            {
                "title": "Matryoshka Networks: Predicting 3D Geometry via Nested Shape Layers",
                "year": 2018,
                "citations": 139,
                "abstract": "In this paper, we develop novel, efficient 2D encodings for 3D geometry, which enable reconstructing full 3D shapes from a single image at high resolution. The key idea is to pose 3D shape reconstruction as a 2D prediction problem. To that end, we first develop a simple baseline network that predicts entire voxel tubes at each pixel of a reference view. By leveraging well-proven architectures for 2D pixel-prediction tasks, we attain state-of-the-art results, clearly outperforming purely voxel-based approaches. We scale this baseline to higher resolutions by proposing a memory-efficient shape encoding, which recursively decomposes a 3D shape into nested shape layers, similar to the pieces of a Matryoshka doll. This allows reconstructing highly detailed shapes with complex topology, as demonstrated in extensive experiments; we clearly outperform previous octree-based approaches despite having a much simpler architecture using standard network components. Our Matryoshka networks further enable reconstructing shapes from IDs or shape similarity, as well as shape sampling.",
                "abstract_summary": "The study presents a novel method for 3D shape reconstruction from a single image using efficient 2D encodings. The researchers developed a baseline network that predicts voxel tubes at each pixel, achieving superior results compared to voxel-based approaches. They further improved the resolution by proposing a memory-efficient shape encoding, which decomposes a 3D shape into nested layers, similar to a Matryoshka doll, outperforming previous octree-based approaches."
            },
            {
                "title": "Depth Pro: Sharp Monocular Metric Depth in Less Than a Second",
                "year": 2024,
                "citations": 106,
                "abstract": "We present a foundation model for zero-shot metric monocular depth estimation. Our model, Depth Pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. The predictions are metric, with absolute scale, without relying on the availability of metadata such as camera intrinsics. And the model is fast, producing a 2.25-megapixel depth map in 0.3 seconds on a standard GPU. These characteristics are enabled by a number of technical contributions, including an efficient multi-scale vision transformer for dense prediction, a training protocol that combines real and synthetic datasets to achieve high metric accuracy alongside fine boundary tracing, dedicated evaluation metrics for boundary accuracy in estimated depth maps, and state-of-the-art focal length estimation from a single image. Extensive experiments analyze specific design choices and demonstrate that Depth Pro outperforms prior work along multiple dimensions. We release code and weights at https://github.com/apple/ml-depth-pro",
                "abstract_summary": "The study introduces a model called Depth Pro for zero-shot metric monocular depth estimation, which creates high-resolution depth maps with sharp details and absolute scale, without needing metadata like camera intrinsics. The model uses a multi-scale vision transformer for dense prediction, a training protocol combining real and synthetic datasets, and new evaluation metrics for boundary accuracy in depth maps. The research shows that Depth Pro surpasses previous work in various aspects."
            },
            {
                "title": "Enhancing Photorealism Enhancement",
                "year": 2021,
                "citations": 106,
                "abstract": "We present an approach to enhancing the realism of synthetic images. The images are enhanced by a convolutional network that leverages intermediate representations produced by conventional rendering pipelines. The network is trained via a novel adversarial objective, which provides strong supervision at multiple perceptual levels. We analyze scene layout distributions in commonly used datasets and find that they differ in important ways. We hypothesize that this is one of the causes of strong artifacts that can be observed in the results of many prior methods. To address this we propose a new strategy for sampling image patches during training. We also introduce multiple architectural improvements in the deep network modules used for photorealism enhancement. We confirm the benefits of our contributions in controlled experiments and report substantial gains in stability and realism in comparison to recent image-to-image translation methods and a variety of other baselines.",
                "abstract_summary": "The research proposes a new approach to improve the realism of synthetic images using a convolutional network and a novel adversarial objective. The study identifies scene layout discrepancies in commonly used datasets as a potential cause of artifacts in previous methods. The researchers introduce a new strategy for image patch sampling during training and architectural improvements in the network modules, which resulted in significant gains in stability and realism compared to other methods."
            },
            {
                "title": "Discriminative shape from shading in uncalibrated illumination",
                "year": 2015,
                "citations": 59,
                "abstract": "Estimating surface normals from just a single image is challenging. To simplify the problem, previous work focused on special cases, including directional lighting, known reflectance maps, etc., making shape from shading impractical outside the lab. To cope with more realistic settings, shading cues need to be combined and generalized to natural illumination. This significantly increases the complexity of the approach, as well as the number of parameters that require tuning. Enabled by a new large-scale dataset for training and analysis, we address this with a discriminative learning approach to shape from shading, which uses regression forests for efficient pixel-independent prediction and fast learning. Von Mises-Fisher distributions in the leaves of each tree enable the estimation of surface normals. To account for their expected spatial regularity, we introduce spatial features, including texton and silhouette features. The proposed silhouette features are computed from the occluding contours of the surface and provide scale-invariant context. Aside from computational efficiency, they enable good generalization to unseen data and importantly allow for a robust estimation of the reflectance map, extending our approach to the uncalibrated setting. Experiments show that our discriminative approach outperforms state-of-the-art methods on synthetic and real-world datasets.",
                "abstract_summary": "The study presents a new approach to estimating surface normals from a single image, using a discriminative learning method called 'shape from shading'. The method uses regression forests and introduces spatial features, including texton and silhouette features, for efficient prediction and learning. The study demonstrates that this approach outperforms existing methods on both synthetic and real-world datasets."
            },
            {
                "title": "Dancing under the stars: video denoising in starlight",
                "year": 2022,
                "citations": 51,
                "abstract": "Imaging in low light is extremely challenging due to low photon counts. Using sensitive CMOS cameras, it is currently possible to take videos at night under moonlight (0.05-0.3 lux illumination). In this paper, we demonstrate photorealistic video under starlight (no moon present, <0.001 lux) for the first time. To enable this, we develop a GAN-tuned physics-based noise model to more accurately represent camera noise at the lowest light levels. Using this noise model, we train a video denoiser using a combination of simulated noisy video clips and real noisy still images. We capture a 5\u201310 fps video dataset with significant motion at approximately 0.6-0.7 millilux with no active illumination. Comparing against alternative methods, we achieve improved video quality at the lowest light levels, demonstrating photorealistic video denoising in starlight for the first time.",
                "abstract_summary": "The research focuses on developing a photorealistic video under starlight (<0.001 lux) using a GAN-tuned physics-based noise model. The model is trained using simulated noisy video clips and real noisy still images. The study demonstrates improved video quality at extremely low light levels, achieving photorealistic video denoising in starlight."
            },
            {
                "title": "Looking Beyond Single Images for Contrastive Semantic Segmentation Learning",
                "year": 2021,
                "citations": 39,
                "abstract": null,
                "abstract_summary": "Apologies, but you haven't provided any abstract for me to summarize. Please provide the text you'd like summarized."
            }
        ]
    },
    {
        "name": "Ama\u00ebl Delaunoy",
        "affiliations": [],
        "citation_count": 466,
        "h_index": 11,
        "papers": [
            {
                "title": "Depth Pro: Sharp Monocular Metric Depth in Less Than a Second",
                "year": 2024,
                "citations": 106,
                "abstract": "We present a foundation model for zero-shot metric monocular depth estimation. Our model, Depth Pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. The predictions are metric, with absolute scale, without relying on the availability of metadata such as camera intrinsics. And the model is fast, producing a 2.25-megapixel depth map in 0.3 seconds on a standard GPU. These characteristics are enabled by a number of technical contributions, including an efficient multi-scale vision transformer for dense prediction, a training protocol that combines real and synthetic datasets to achieve high metric accuracy alongside fine boundary tracing, dedicated evaluation metrics for boundary accuracy in estimated depth maps, and state-of-the-art focal length estimation from a single image. Extensive experiments analyze specific design choices and demonstrate that Depth Pro outperforms prior work along multiple dimensions. We release code and weights at https://github.com/apple/ml-depth-pro",
                "abstract_summary": "The study introduces a new model, Depth Pro, for zero-shot metric monocular depth estimation, capable of creating high-resolution depth maps quickly and without needing metadata. The model's efficiency is due to several technical contributions, including a multi-scale vision transformer, a training protocol combining real and synthetic datasets, and new evaluation metrics. The research demonstrates that Depth Pro surpasses previous models in performance."
            },
            {
                "title": "Photometric Bundle Adjustment for Dense Multi-view 3D Modeling",
                "year": 2014,
                "citations": 105,
                "abstract": null,
                "abstract_summary": "Apologies, but you didn't provide any text to summarize. Please provide an abstract or a text for me to summarize."
            },
            {
                "title": "Minimizing the Multi-view Stereo Reprojection Error for Triangular Surface Meshes",
                "year": 2008,
                "citations": 80,
                "abstract": "This article proposes a variational multi-view stereo vision method based on meshes for recovering 3D scenes (shape and radiance) from images. Our method is based on generative models and minimizes the reprojection error (difference between the observed images and the images synthesized from the reconstruction). Our contributions are twofold. 1) For the first time, we rigorously compute the gradient of the reprojection error for non smooth surfaces defined by discrete triangular meshes. The gradient correctly takes into account the visibility changes that occur when a surface moves; this forces the contours generated by the reconstructed surface to perfectly match with the apparent contours in the input images. 2) We propose an original modification of the Lambertian model to take into account deviations from the constant brightness assumption without explicitly modelling the reflectance properties of the scene or other photometric phenomena involved by the camera model. Our method is thus able to recover the shape and the diffuse radiance of non Lambertian scenes.",
                "abstract_summary": "The research introduces a novel variational multi-view stereo vision method for 3D scene recovery from images, using generative models and minimizing reprojection error. The study innovatively computes the gradient of the reprojection error for non-smooth surfaces defined by discrete triangular meshes. It also modifies the Lambertian model to account for deviations from constant brightness without explicitly modeling reflectance properties or other photometric phenomena."
            },
            {
                "title": "Gradient Flows for Optimizing Triangular Mesh-based Surfaces: Applications to 3D Reconstruction Problems Dealing with\u00a0Visibility",
                "year": 2011,
                "citations": 63,
                "abstract": null,
                "abstract_summary": "Apologies, but there seems to be no abstract provided to summarize. Please provide the text for summarization."
            },
            {
                "title": "Towards Full 3D Helmholtz Stereovision Algorithms",
                "year": 2010,
                "citations": 21,
                "abstract": null,
                "abstract_summary": "Apologies, but I can't provide a summary as there's no abstract provided. Please provide the abstract you'd like summarized."
            },
            {
                "title": "Colour Dynamic Photometric Stereo for Textured Surfaces",
                "year": 2010,
                "citations": 20,
                "abstract": null,
                "abstract_summary": "Apologies, but you didn't provide any abstract for me to summarize. Please provide the text you would like summarized."
            },
            {
                "title": "Automatic 3D reconstruction of manifold meshes via delaunay triangulation and mesh sweeping",
                "year": 2016,
                "citations": 18,
                "abstract": "In this paper we propose a new approach to incrementally initialize a manifold surface for automatic 3D reconstruction from images. More precisely we focus on the automatic initialization of a 3D mesh as close as possible to the final solution; indeed many approaches require a good initial solution for further refinement via multi-view stereo techniques. Our novel algorithm automatically estimates an initial manifold mesh for surface evolving multi-view stereo algorithms, where the manifold property needs to be enforced. It bootstraps from 3D points extracted via Structure from Motion, then iterates between a state-of-the-art manifold reconstruction step and a novel mesh sweeping algorithm that looks for new 3D points in the neighborhood of the reconstructed manifold to be added in the manifold reconstruction. The experimental results show quantitatively that the mesh sweeping improves the resolution and the accuracy of the manifold reconstruction, allowing a better convergence of state-of-the-art surface evolution multi-view stereo algorithms.",
                "abstract_summary": "This paper presents a novel algorithm for automatic 3D reconstruction from images, focusing on the initialization of a 3D mesh. The algorithm bootstraps from 3D points extracted via Structure from Motion and alternates between manifold reconstruction and a new mesh sweeping technique. Experimental results demonstrate that this approach improves the resolution and accuracy of the manifold reconstruction, enhancing the performance of multi-view stereo algorithms."
            },
            {
                "title": "Convex multi-region segmentation on manifolds",
                "year": 2009,
                "citations": 17,
                "abstract": null,
                "abstract_summary": "Apologies, but there seems to be no abstract provided for me to summarize. Please provide the abstract you'd like me to summarize."
            },
            {
                "title": "Multiview projectors/cameras system for 3D reconstruction of dynamic scenes",
                "year": 2011,
                "citations": 14,
                "abstract": null,
                "abstract_summary": "Apologies, but you haven't provided any abstract for me to summarize. Please provide the text you want summarized."
            },
            {
                "title": "Biaxial Liquid Crystal Elastomers: a Lattice Model",
                "year": 2008,
                "citations": 13,
                "abstract": null,
                "abstract_summary": "Apologies, but there seems to be no content provided for me to summarize. Please provide an abstract or text for me to assist you."
            }
        ]
    }
]